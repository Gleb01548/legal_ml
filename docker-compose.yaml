services:
  vector_db:
    image: qdrant/qdrant:latest
    container_name: vector_db
    restart: always
    volumes:
      - /home/ssd/volumes/qdrant:/qdrant/storage
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    networks:
      - default
  # embedding_model:
  #   image: ghcr.io/huggingface/text-embeddings-inference:1.6
  #   container_name: embedding_model
  #   command: --model-id BAAI/bge-m3 --max-client-batch-size=1024 --max-batch-requests=1024 --max-batch-tokens=64000
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1 # alternatively, use `count: all` for all GPUs
  #             capabilities: [gpu]
  #   volumes:
  #     - ./embedding_model_data:/data
  #   ports:
  #     - 8080:80
  #   networks:
  #     - default
  # llm:
  #   image: vllm/vllm-openai
  #   container_name: llms
  #   restart: always
  #   ports:
  #     - "5000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0']
  #             capabilities: [gpu]
  #   ipc: host
  #   command: --model unsloth/Qwen2.5-7B-Instruct --max_seq_len=1000 --max_model_len=6000 --enable_prefix_caching # --api-key=empty
  #   networks:
  #     - default