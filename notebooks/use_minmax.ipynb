{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"src\" not in os.listdir():\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conf.yaml') as fh:\n",
    "    read_data = yaml.load(fh, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['URL_DB_QDRANT', 'API_DB_QDRANT'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://api.minimaxi.chat/v1/text/chatcompletion_v2\"\n",
    "# API_KEY = read_data[\"api_minmax\"]\n",
    "# payload = json.dumps({\n",
    "#     \"model\":\"MiniMax-Text-01\",\n",
    "#     \"messages\":[\n",
    "#         {\n",
    "#             \"role\":\"system\",\n",
    "#             \"content\":\"Ты профессиональный юрист, специалист по российскому праву. Ничего не придумывай. В конце ответа пиши краткое резюме своего ответа, где указывай прямой и сжатый ответ на впрос пользователя.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\":\"user\",\n",
    "#             \"content\":\"Что делать если не берут а Армию? Я очень хочу служить России.\"\n",
    "#         }\n",
    "#     ],\n",
    "#     \"stream\":False,\n",
    "#     \"max_tokens\":10000,\n",
    "#     \"temperature\":0.9,\n",
    "#     \"top_p\":1\n",
    "# })\n",
    "# headers = {\n",
    "#     'Authorization': f'Bearer {API_KEY}',\n",
    "#     'Content-Type': 'application/json'\n",
    "# }\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'api_mts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://demo5-fundres.dev.mts.ai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_mts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcotype_pro_16k_1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ],\n\u001b[1;32m     15\u001b[0m })\n\u001b[1;32m     16\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'api_mts'"
     ]
    }
   ],
   "source": [
    "url = \"https://demo5-fundres.dev.mts.ai/v1/chat/completions\"\n",
    "API_KEY = read_data[\"api_mts\"]\n",
    "payload = json.dumps({\n",
    "    \"model\":\"cotype_pro_16k_1.1\",\n",
    "    \"messages\":[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"Ты модель AI. Пиши коротко.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"Кто ты?\"\n",
    "        }\n",
    "    ],\n",
    "})\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.yandex import ChatYandexGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yandex.cloud.ai.foundation_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myandex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_models\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yandex.cloud.ai.foundation_models'"
     ]
    }
   ],
   "source": [
    "import yandex.cloud.ai.foundation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True, device=\"cpu\") # Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute Scores: 100%|██████████| 1/1 [01:31<00:00, 91.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658, -5.822876453399658]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = reranker.compute_score([['query', 'passage passage'*10] for _ in range(200)])\n",
    "print(score) # -5.65234375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatOpenAI(model_name=\"cotype_pro_16k_1.1\", base_url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.generate([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Dict, Union\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema import Generation, LLMResult\n",
    "import requests\n",
    "\n",
    "class CustomLLM(BaseLLM):\n",
    "    api_url: str  # URL вашего API\n",
    "    api_key: str  # API ключ, если требуется\n",
    "    model_name: str = \"cotype_pro_16k_1.1\"  # Имя модели\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Системное сообщение\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Ты ассистент, который помогает людям. Пиши на русском языке!\"\n",
    "        }\n",
    "        # Пользовательское сообщение\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [system_message, user_message],\n",
    "        }\n",
    "        # Отправка запроса к API\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Извлечение ответа из результатов\n",
    "        # Предполагается, что ответ находится в result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        assistant_response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return assistant_response\n",
    "\n",
    "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            text = self._call(prompt, stop)\n",
    "            generations.append([Generation(text=text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chat_api = CustomLLM(\n",
    "    api_url=\"https://demo5-fundres.dev.mts.ai/v1/chat/completions\",\n",
    "    model_name=\"cotype_pro_16k_1.1\",\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chat_api.invoke(\"кто ты?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
